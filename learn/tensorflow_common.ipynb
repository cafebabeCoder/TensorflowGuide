{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([1, 2])\n",
    "a = tf.constant([3, 3])\n",
    "\n",
    "add = tf.add(x, a)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name='counter')\n",
    "iters = tf.add(state, 1)\n",
    "update = tf.assign(state, iters)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(state))\n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch & feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "#fetch可以执行多个操作\n",
    "input1 = tf.constant(3.0)\n",
    "input2 = tf.constant(2.0)\n",
    "add = tf.add(input1, input2)\n",
    "mul = tf.multiply(input1, input2)\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([add, mul])\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2]\n"
     ]
    }
   ],
   "source": [
    "#feed可以在运行的时候传入值\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.multiply(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output, feed_dict={input1:[0.5], input2:[0.4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [0.05746142, 0.101494886])\n",
      "(20, [0.10650051, 0.19629987])\n",
      "(40, [0.10413487, 0.19764642])\n",
      "(60, [0.10263014, 0.19850293])\n",
      "(80, [0.101673, 0.19904773])\n",
      "(100, [0.10106417, 0.19939427])\n",
      "(120, [0.10067691, 0.1996147])\n",
      "(140, [0.10043056, 0.19975492])\n",
      "(160, [0.10027386, 0.1998441])\n",
      "(180, [0.10017419, 0.19990085])\n"
     ]
    }
   ],
   "source": [
    "x_data = np.random.rand(100)\n",
    "y_data = x_data * 0.1 + 0.2\n",
    "\n",
    "#构造一个线性模型\n",
    "b = tf.Variable(0.)\n",
    "k = tf.Variable(0.)\n",
    "y = k*x_data + b\n",
    "loss = tf.reduce_mean(tf.square(y_data - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "train = optimizer.minimize(loss)\n",
    "#init\n",
    "init = tf.global_variables_initializer()\n",
    "#run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(200):\n",
    "        sess.run(train)\n",
    "        if(step % 20 == 0):\n",
    "            print(step, sess.run([k, b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhIf/A0tg98nzg+BhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffacc699210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#生成样本点\n",
    "x_data = np.linspace(-0.5, 0.5, 200)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.02, x_data.shape)\n",
    "y_data = np.square(x_data) + noise\n",
    "#network\n",
    "x = tf.placeholder(tf.float32, [None, 1])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#L1\n",
    "w1 = tf.Variable(tf.random_normal([1, 10]))\n",
    "b1 = tf.Variable(tf.zeros([1, 10]))\n",
    "a1 = tf.matmul(x, w1) + b1\n",
    "L1 = tf.tanh(a1)\n",
    "\n",
    "#L2\n",
    "w2 = tf.Variable(tf.random_normal([10, 1]))\n",
    "b2 = tf.Variable(tf.zeros([1, 1]))\n",
    "a2 = tf.matmul(L1, w2) + b2\n",
    "out = tf.tanh(a2)\n",
    "\n",
    "#train\n",
    "loss = tf.reduce_mean(tf.square(out - y_data))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for _ in range(2000):\n",
    "            sess.run(train, feed_dict={x:x_data, y:y_data})\n",
    "        \n",
    "        predict_v = sess.run(out, feed_dict={x:x_data})\n",
    "        \n",
    "        #draw\n",
    "        plt.figure()\n",
    "        plt.scatter(x_data, y_data)\n",
    "        plt.plot(x_data, predict_v, 'r-')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小二乘和交叉熵两种loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, accuracy: 0.5487\n",
      "Iter: 1, accuracy: 0.6145\n",
      "Iter: 2, accuracy: 0.7047\n",
      "Iter: 3, accuracy: 0.7683\n",
      "Iter: 4, accuracy: 0.7904\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST/MNIST_data\", one_hot=True)\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#network\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "predict = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "#两种loss,最小二乘和交叉熵\n",
    "#loss = tf.reduce_mean(tf.square(y-predict))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=predict))\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "#run\n",
    "init = tf.global_variables_initializer()\n",
    "correct_predict = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1))  #argmax返回最大值的位置\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        auc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "        print(\"Iter: \"+str(epoch) +\", accuracy: \"+str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop-out & init\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\workPlace\\python\\MNIST\\MNIST_data\\/train-images-idx3-ubyte.gz\n",
      "Extracting D:\\workPlace\\python\\MNIST\\MNIST_data\\/train-labels-idx1-ubyte.gz\n",
      "Extracting D:\\workPlace\\python\\MNIST\\MNIST_data\\/t10k-images-idx3-ubyte.gz\n",
      "Extracting D:\\workPlace\\python\\MNIST\\MNIST_data\\/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, test_acc: 0.411, train_acc: 0.4158\n",
      "Iter: 1, test_acc: 0.5667273, train_acc: 0.5739\n",
      "Iter: 2, test_acc: 0.64525455, train_acc: 0.6563\n",
      "Iter: 3, test_acc: 0.7175091, train_acc: 0.7213\n",
      "Iter: 4, test_acc: 0.75714546, train_acc: 0.7631\n",
      "Iter: 5, test_acc: 0.7824182, train_acc: 0.7949\n",
      "Iter: 6, test_acc: 0.7994, train_acc: 0.8099\n",
      "Iter: 7, test_acc: 0.81094545, train_acc: 0.8208\n",
      "Iter: 8, test_acc: 0.8232727, train_acc: 0.8294\n",
      "Iter: 9, test_acc: 0.8293273, train_acc: 0.8411\n",
      "Iter: 10, test_acc: 0.83216363, train_acc: 0.8412\n",
      "Iter: 11, test_acc: 0.8421636, train_acc: 0.8476\n",
      "Iter: 12, test_acc: 0.84385455, train_acc: 0.8489\n",
      "Iter: 13, test_acc: 0.84647274, train_acc: 0.8531\n",
      "Iter: 14, test_acc: 0.8526545, train_acc: 0.8608\n",
      "Iter: 15, test_acc: 0.85449094, train_acc: 0.8632\n",
      "Iter: 16, test_acc: 0.8574909, train_acc: 0.8645\n",
      "Iter: 17, test_acc: 0.8607091, train_acc: 0.8672\n",
      "Iter: 18, test_acc: 0.8623273, train_acc: 0.8655\n",
      "Iter: 19, test_acc: 0.86545455, train_acc: 0.8694\n",
      "Iter: 20, test_acc: 0.8676, train_acc: 0.8778\n",
      "Iter: 21, test_acc: 0.8685455, train_acc: 0.8731\n",
      "Iter: 22, test_acc: 0.86963636, train_acc: 0.8782\n",
      "Iter: 23, test_acc: 0.87118185, train_acc: 0.8763\n",
      "Iter: 24, test_acc: 0.87381816, train_acc: 0.8804\n",
      "Iter: 25, test_acc: 0.8745091, train_acc: 0.8786\n",
      "Iter: 26, test_acc: 0.8763091, train_acc: 0.8803\n",
      "Iter: 27, test_acc: 0.8763273, train_acc: 0.8822\n",
      "Iter: 28, test_acc: 0.8789273, train_acc: 0.8818\n",
      "Iter: 29, test_acc: 0.87805456, train_acc: 0.8837\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST/MNIST_data\", one_hot=True)\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#network\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.truncated_normal([784, 400], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([400]) + 0.1)\n",
    "L1 = tf.tanh(tf.matmul(x, w1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([400, 400], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([400]) + 0.1)\n",
    "L2 = tf.tanh(tf.matmul(L1_drop, w2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([400, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "predict = tf.nn.softmax(tf.matmul(L2_drop, w3) + b3)\n",
    "\n",
    "#两种loss,最小二乘和交叉熵\n",
    "#loss = tf.reduce_mean(tf.square(y-predict))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=predict))\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "#run\n",
    "init = tf.global_variables_initializer()\n",
    "correct_predict = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1))  #argmax返回最大值的位置\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(30):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x:batch_xs, y:batch_ys,keep_prob:0.5})\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:0.5})\n",
    "        train_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:0.5})\n",
    "        print(\"Iter: \"+str(epoch) +\", test_acc: \"+str(test_acc) + \", train_acc: \"+ str(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5c39e4312b03>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /data/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /data/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /data/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /data/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /data/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-2-5c39e4312b03>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Iter: 0 acc: 0.867\n",
      "Iter: 1 acc: 0.9692\n",
      "Iter: 2 acc: 0.9789\n",
      "Iter: 3 acc: 0.983\n",
      "Iter: 4 acc: 0.9848\n",
      "Iter: 5 acc: 0.9866\n",
      "Iter: 6 acc: 0.9876\n",
      "Iter: 7 acc: 0.9868\n",
      "Iter: 8 acc: 0.9884\n",
      "Iter: 9 acc: 0.9899\n",
      "Iter: 10 acc: 0.9902\n",
      "Iter: 11 acc: 0.9905\n",
      "Iter: 12 acc: 0.9904\n",
      "Iter: 13 acc: 0.9896\n",
      "Iter: 14 acc: 0.9917\n",
      "Iter: 15 acc: 0.9908\n",
      "Iter: 16 acc: 0.9903\n",
      "Iter: 17 acc: 0.9918\n",
      "Iter: 18 acc: 0.993\n",
      "Iter: 19 acc: 0.9917\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST/MNIST_data', one_hot=True)\n",
    "\n",
    "#定义批从\n",
    "batch_size = 100\n",
    "n_batchs = mnist.train.num_examples // batch_size\n",
    "\n",
    "#初始化权重\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#初始化偏置\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x, W):\n",
    "    #input tensor [batch, in_height, in_width, in_channels]\n",
    "    #strides : 步长 strides[0]=strides[3]=1, strides[1]:x步长， strides[2]:y 步长\n",
    "    #padding \"SAME\" \"VALID\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize [1, x, y, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "#nn\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#CNN需要2维数据 [batch, height, width, channels]\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "#L1\n",
    "w_conv1 = weight_variable([5, 5, 1, 32]) # 5 * 5的窗口，从一个平面上抽32个平面\n",
    "b_conv1 = bias_variable([32])  #每个平面加一个bias\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)  #x_image和权值向量卷积，加上偏置，再加relu\n",
    "h_pool1 = max_pool_2x2(h_conv1)   #池化\n",
    "\n",
    "#L2\n",
    "w_conv2 = weight_variable([5, 5, 32, 64]) # 5 * 5的窗口，从一个平面上抽32个平面\n",
    "b_conv2 = bias_variable([64])  #每个平面加一个bias\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#1st:28*28 卷积后:28*28, 池化后：14*14\n",
    "#2nd:14*14  卷积后:14*14, 池化后: 7*7\n",
    "#最后得到 64张 7*7 的平面\n",
    "\n",
    "#全连接层\n",
    "#L1\n",
    "w_fc_1 = weight_variable([7*7*64, 1024])  #把7*7*64的结果平铺为1层\n",
    "b_fc_1 = weight_variable([1024])\n",
    "drop_out = tf.placeholder(tf.float32)  #dropout\n",
    "\n",
    "h_pool2_1d = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc_out_1 = tf.nn.relu(tf.matmul(h_pool2_1d, w_fc_1) + b_fc_1)\n",
    "L1_drop = tf.nn.dropout(h_fc_out_1, drop_out)\n",
    "\n",
    "#L2\n",
    "w_fc_2 = weight_variable([1024, 10])\n",
    "b_fc_2 = bias_variable([10])\n",
    "\n",
    "#train\n",
    "predict = tf.nn.softmax(tf.matmul(L1_drop, w_fc_2) + b_fc_2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict))\n",
    "train = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_predict = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "\n",
    "#run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(20):\n",
    "        for batch in range(n_batchs):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x:batch_xs, y:batch_ys, drop_out:0.7})\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y: mnist.test.labels, drop_out:1.0})\n",
    "        print(\"Iter: \" + str(epoch)+\" acc: \"+ str(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1471658fb88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../MNIST/MNIST_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#定义批从\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_batchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST/MNIST_data', one_hot=True)\n",
    "\n",
    "#定义批从\n",
    "batch_size = 100\n",
    "n_batchs = mnist.train.num_examples // batch_size\n",
    "\n",
    "#参数\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.scalar('histogram', var)\n",
    "\n",
    "#初始化权重\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "#初始化偏置\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "#卷积层\n",
    "def conv2d(x, W):\n",
    "    #input tensor [batch, in_height, in_width, in_channels]\n",
    "    #strides : 步长 strides[0]=strides[3]=1, strides[1]:x步长， strides[2]:y 步长\n",
    "    #padding \"SAME\" \"VALID\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize [1, x, y, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    #nn\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_input')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y_input')\n",
    "    with tf.name_scope(\"x_image\"):\n",
    "    #CNN需要2维数据 [batch, height, width, channels]\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1], name='x_image')\n",
    "\n",
    "#L1\n",
    "with tf.name_scope('conv1'):\n",
    "    #初始化weight和bias\n",
    "    with tf.name_scope('w_conv1'):\n",
    "        w_conv1 = weight_variable([5, 5, 1, 32], name='w_conv1') # 5 * 5的窗口，从一个平面上抽32个平面\n",
    "    with tf.name_scope('b_conv1'):\n",
    "        b_conv1 = bias_variable([32], name='b_conv1')  #每个平面加一个bias\n",
    "    #卷积\n",
    "    with tf.name_scope('conv2d_1'):\n",
    "        conv2d_1 = conv2d(x_image, w_conv1) + b_conv1  #x_image和权值向量卷积，加上偏置，再加relu\n",
    "    with tf.name_scope('relu'):\n",
    "        h_conv1 = tf.nn.relu(conv2d_1)\n",
    "    #池化\n",
    "    with tf.name_scope('h_pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)   #池化\n",
    "\n",
    "#L2\n",
    "with tf.name_scope('conv2'):\n",
    "    #初始化weight & bias\n",
    "    with tf.name_scope('w_conv2'):\n",
    "        w_conv2 = weight_variable([5, 5, 32, 64], name='w_conv2') # 5 * 5的窗口，从一个平面上抽32个平面\n",
    "    with tf.name_scope('b_conv2'):\n",
    "        b_conv2 = bias_variable([64], name='b_conv2')  #每个平面加一个bias\n",
    "    #卷积\n",
    "    with tf.name_scope('conv2d_2'):\n",
    "        conv2d_2 = conv2d(h_pool1, w_conv2) + b_conv2\n",
    "    with tf.name_scope('relu'):\n",
    "        h_conv2 = tf.nn.relu(conv2d_2)\n",
    "    #池化\n",
    "    with tf.name_scope('h_pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#1st:28*28 卷积后:28*28, 池化后：14*14\n",
    "#2nd:14*14  卷积后:14*14, 池化后: 7*7\n",
    "#最后得到 64张 7*7 的平面\n",
    "\n",
    "#全连接层\n",
    "with tf.name_scope('fc1'):\n",
    "#L1\n",
    "    #初始化weight & bias\n",
    "    with tf.name_scope('w_fc1'):\n",
    "        w_fc_1 = weight_variable([7*7*64, 1024], name='w_fc1')  \n",
    "    with tf.name_scope('b_fc1'):\n",
    "        b_fc_1 = weight_variable([1024], name='b_fc1')\n",
    "    #把7*7*64的结果平铺为1层\n",
    "    with tf.name_scope('h_pool_flat1'):  \n",
    "        h_pool2_1d = tf.reshape(h_pool2, [-1, 7 * 7 * 64], name='h_pool_flat1')\n",
    "    #tanh(wx + b)\n",
    "    with tf.name_scope('fc_out'):\n",
    "        h_fc_a_1 = tf.matmul(h_pool2_1d, w_fc_1) + b_fc_1\n",
    "    with tf.name_scope('relu'):\n",
    "        h_fc_out_1 = tf.nn.relu(h_fc_a_1)\n",
    "    #加一层drop-out\n",
    "    with tf.name_scope('drop_out'):\n",
    "        drop_out = tf.placeholder(tf.float32, name='dropout')  #dropout\n",
    "    with tf.name_scope('out_1'):\n",
    "        L1_drop = tf.nn.dropout(h_fc_out_1, drop_out, name='out_1')\n",
    "\n",
    "#L2\n",
    "with tf.name_scope('fc2'):\n",
    "    #初始化weight & bias\n",
    "    with tf.name_scope('w_fc2'):\n",
    "        w_fc_2 = weight_variable([1024, 10], name='w_fc2')\n",
    "    with tf.name_scope('b_fc2'):\n",
    "        b_fc_2 = bias_variable([10], name='b_fc2')\n",
    "    # softmax(w*x + b)\n",
    "    with tf.name_scope('fc_out2'):\n",
    "        h_fc_out_2 = tf.matmul(L1_drop, w_fc_2) + b_fc_2\n",
    "    with tf.name_scope('softmax'):\n",
    "        predict = tf.nn.softmax(h_fc_out_2)\n",
    "\n",
    "#loss function\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict), name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entroy', cross_entropy)\n",
    "with tf.name_scope('train'):\n",
    "    train = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_predict'):\n",
    "        correct_predict = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        \n",
    "merge = tf.summary.merge_all()\n",
    "        \n",
    "#run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test', sess.graph)\n",
    "    for epoch in range(200):\n",
    "        #train\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train, feed_dict={x:batch_xs, y:batch_ys, drop_out:0.7})\n",
    "\n",
    "        summary = sess.run(merge, feed_dict={x:batch_xs, y:batch_ys, drop_out:0.7})\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "        #test\n",
    "        batch_xs, batch_ys = mnist.test.next_batch(batch_size)            \n",
    "        summary = sess.run(merge, feed_dict={x:batch_xs, y:batch_ys, drop_out:0.7})\n",
    "        test_writer.add_summary(summary, epoch)\n",
    "\n",
    "        if(epoch % 100 ==0):\n",
    "            test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y: mnist.test.labels, drop_out:1.0})\n",
    "            train_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y: mnist.test.labels, drop_out:1.0})\n",
    "            print(\"Iter: \" + str(epoch)+\" test acc: \"+ str(test_acc) + \" train acc: \" + str(train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, test acc: 0.58\n",
      "Iter: 1, test acc: 0.84\n",
      "Iter: 2, test acc: 0.88\n",
      "Iter: 3, test acc: 0.92\n",
      "Iter: 5, test acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST/MNIST_data', one_hot=True)\n",
    "tf.reset_default_graph()\n",
    "#图片是28*28\n",
    "n_input = 28 #一行，一行有28个数据\n",
    "max_time = 28 #一共28行\n",
    "lstm_size =100 #隐藏单元\n",
    "n_classes = 10 #10个分类\n",
    "batch_size = 50 #每批次50个样本\n",
    "n_batch = mnist.train.num_examples //  batch_size #计算批次\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])  #标签\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size, n_classes], stddev = 0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[n_classes]))\n",
    "\n",
    "#定义RNN\n",
    "def RNN1(X, weights, bias):\n",
    "    inputs = tf.reshape(X, [-1, max_time, n_input])\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    # final_state[0]是cell state\n",
    "    # final_state[1]是hidden state\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, inputs, dtype = tf.float32)\n",
    "    results = tf.nn.softmax(tf.matmul(final_state[1], weights) + bias)\n",
    "    return results\n",
    "\n",
    "#RNN的结果\n",
    "prediction = RNN1(x, weights, bias)\n",
    "#损失函数\n",
    "cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "#train\n",
    "train = tf.train.AdamOptimizer(1e-4).minimize(cross_entroy)\n",
    "#结果放在bool类型中\n",
    "correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(prediction, 1))\n",
    "#准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#初始化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(6):\n",
    "        for batch in range(n_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x:batch_x, y:batch_y})\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict={x:batch_x, y:batch_y})\n",
    "        print(\"Iter: \"+str(epoch)+\", test acc: \"+str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter: 0, accuracy: 0.5479\n",
      "Iter: 1, accuracy: 0.6138\n",
      "Iter: 2, accuracy: 0.7031\n",
      "Iter: 3, accuracy: 0.765\n",
      "Iter: 4, accuracy: 0.7906\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST/MNIST_data\", one_hot=True)\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#network\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "predict = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "#两种loss,最小二乘和交叉熵\n",
    "#loss = tf.reduce_mean(tf.square(y-predict))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=predict))\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "#run\n",
    "init = tf.global_variables_initializer()\n",
    "correct_predict = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1))  #argmax返回最大值的位置\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train, feed_dict={x:batch_xs, y:batch_ys})\n",
    "        auc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})\n",
    "        print(\"Iter: \"+str(epoch) +\", accuracy: \"+str(auc))\n",
    "        \n",
    "    #save\n",
    "    saver.save(sess, 'models/mnist_model_save.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.098\n",
      "INFO:tensorflow:Restoring parameters from models/mnist_model_save.ckpt\n",
      "0.098\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST/MNIST_data\", one_hot=True)\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#network\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "predict = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "#两种loss,最小二乘和交叉熵\n",
    "#loss = tf.reduce_mean(tf.square(y-predict))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits=predict))\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "#run\n",
    "init = tf.global_variables_initializer()\n",
    "correct_predict = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1))  #argmax返回最大值的位置\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))\n",
    "    saver.restore(sess, 'models/mnist_model_save.ckpt')\n",
    "    print(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
